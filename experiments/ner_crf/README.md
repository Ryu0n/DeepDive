# Custom_NER

- ğŸ¤—`Huggingface Tranformers`ğŸ¤— ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ êµ¬í˜„
- KoBERT, KoELECTRA (+ CRF)ë¥¼ ì´ìš©í•œ í•œêµ­ì–´ Named Entity Recognition Task
- ê¸°ì¡´ KoBERTì— CRF(Conditional Random Field)ë¥¼ ë¶™ì¸ ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ì •ì˜
- monologgë‹˜ì˜ KoBERT-NERë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•
- FastAPIë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë¡œë“œ
- ëª¨ë‘ì˜ ë§ë­‰ì¹˜ (./data_preprocess) Fine-Tuning
- í† í° ë‹¨ìœ„ íƒœê¹… ê¸°ëŠ¥ ì¶”ê°€ (í˜•íƒœì†Œ ë¶„ì„ê¸° í’ˆì‚¬ í™œìš©)

## Dependencies
- torch==1.4.0
- transformers==2.10.0
- seqeval>=0.0.12
- pytorch-crf==0.7.2
- fastapi==0.68.1
- uvicorn==0.15.0


## How to use This Repository
1. Traning & Evaluation ê³¼ì •ì„ í†µí•´ í•™ìŠµ ëª¨ë¸ íŒŒì¼ ìƒì„±
2. Prediction ê³¼ì •ì„ í†µí•´ uvicorn ASGI(Async Server Gateway Interface)ê³¼ FastAPI ì›¹ ì„œë²„ë¥¼ êµ¬ë™
3. POST Request

## Training & Evaluation

```bash
# KoBERT, KoELECTRA (for token classification) Only Command
$ CUDA_VISIBLE_DEVICES=0 python main.py --data_dir ./data_preprocess --model_type kobert --do_train --do_eval --write_pred
$ CUDA_VISIBLE_DEVICES=0 python main.py --data_dir ./data_preprocess --model_type koelectra-base-v3  --do_train --do_eval --write_pred

# Custom classes
$ CUDA_VISIBLE_DEVICES=0 python main.py --data_dir ./data_preprocess --model_type kobert-crf --model_dir ./kobert_crf_model --do_train --do_eval --pred_dir ./preds_crf --write_pred

# KoBERT
$ CUDA_VISIBLE_DEVICES=0 python main.py --data_dir ./data_preprocess --model_type kobert-torchcrf --model_dir ./kobert_crf_model --do_train --do_eval --pred_dir ./preds_torchcrf --write_pred
$ CUDA_VISIBLE_DEVICES=0 python main.py --data_dir ./data_preprocess --model_type kobert-lstm-torchcrf --model_dir ./kobert_crf_model --do_train --do_eval --pred_dir ./preds_lstm_torchcrf --write_pred
$ CUDA_VISIBLE_DEVICES=0 python main.py --data_dir ./data_preprocess --model_type kobert-bilstm-torchcrf --model_dir ./kobert_crf_model --do_train --do_eval --pred_dir ./preds_bilstm_torchcrf --write_pred

# KoELECTRA
$ CUDA_VISIBLE_DEVICES=0 python main.py --data_dir ./data_preprocess --model_type koelectra-base-v3-torchcrf --model_dir ./kobert_crf_model --do_train --do_eval --pred_dir ./preds_elec_torchcrf --write_pred
$ CUDA_VISIBLE_DEVICES=0 python main.py --data_dir ./data_preprocess --model_type koelectra-base-v3-lstm-torchcrf --model_dir ./kobert_crf_model --do_train --do_eval --pred_dir ./preds_elec_lstm_torchcrf --write_pred
$ CUDA_VISIBLE_DEVICES=0 python main.py --data_dir ./data_preprocess --model_type koelectra-base-v3-bilstm-torchcrf --model_dir ./kobert_crf_model --do_train --do_eval --pred_dir ./preds_elec_bilstm_torchcrf --write_pred
```

## Prediction
```bash
$ uvicorn app:app --reload
```

```python
# POST REQUEST 
# host_name:port/named_entity_recognition
# Request Body Example
{
  "sentences": [
    "ì„±ê·œëŠ” ëŸ°ë˜ì—ì„œ ë¹µì„ ì¢‹ì•„í•œë‹¤.", 
    "í•œì†Œí¬ëŠ” ì„±ê·œë¥¼ í˜œí™”ë™ì—ì„œ ì¢‹ì•„í•œë‹¤.",
    "ë‚˜ëŠ” ì„œìš¸ì—ì„œ ë„ ë³¸ìˆœê°„ ì‚¬ë‘ì— ë¹ ì¡Œì–´",
    "ë„Œ ëŸ¬ì‹œì•„ì— ìˆëŠ” ì§‘ì— ê°€ì„œ ì ì„ ìê³  ì‹¶ë‹¤",
    "ë°ì´í„° ë¶„ì„ê°€ ìŠˆí¼ë§ˆë¦¬ì˜¤ëŠ” ì í”„ë¥¼ 6ë²ˆ í–ˆë‹¤.",
    "2022ë…„ ë‚˜ëŠ” ì´ë™ìš±ì„ ë¡¤ ëª¨ë¸ë¡œ ì‚¼ì•˜ì§€ë§Œ, í˜„ì‹¤ì€ ê·¸ë ‡ì§€ ì•Šë‹¤."
  ]
}
```

## References
- [monologg/KoBERT-NER](https://github.com/monologg/KoBERT-NER)
- [Implementing a linear-chain Conditional Random Field(CRF) in PyTorch](https://towardsdatascience.com/implementing-a-linear-chain-conditional-random-field-crf-in-pytorch-16b0b9c4b4ea)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
- [ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS](https://arxiv.org/pdf/2003.10555.pdf)
