{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/prefix-tuning/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from prefix_gptneox_model import PrefixGPTNeoXLMHeadModel\n",
    "from args import Args\n",
    "\n",
    "\n",
    "def load_trained_model(model_checkpoint='weights/checkpoints/checkpoint-7760/pytorch_model.bin'):\n",
    "    args = Args()\n",
    "    model = PrefixGPTNeoXLMHeadModel(args)\n",
    "    state_dict = torch.load(model_checkpoint)\n",
    "    model.load_state_dict(state_dict=state_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "regex_emoji = re.compile(':.*:')\n",
    "def remove_emoji(prompt: str):\n",
    "    return regex_emoji.sub('', prompt)\n",
    "\n",
    "\n",
    "def generate_texts(model, prompt: str):\n",
    "    model.eval()\n",
    "    model.to('cuda')\n",
    "    start_tokens = f'<s>{prompt}'\n",
    "    model_in = start_tokens + ' [A] '\n",
    "    inputs = model.tokenizer([model_in], max_length=256, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    inputs.to('cuda')\n",
    "    # print(inputs[\"input_ids\"])\n",
    "    generated_ids = model.generate(inputs[\"input_ids\"], \\\n",
    "            attention_mask = inputs[\"attention_mask\"], \\\n",
    "            num_beams=1, min_length=32, do_sample = False, \\\n",
    "            max_length=63, repetition_penalty = 1.2, no_repeat_ngram_size = 3, early_stopping = True)\n",
    "\n",
    "    result = model.tokenizer.batch_decode(generated_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True)[0]\n",
    "    results = result.split(' [A] ')\n",
    "    print('Q : ', results[0])\n",
    "    for result in results[1:]:\n",
    "        print('A : ', result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix-tuning sequence length is 8.\n"
     ]
    }
   ],
   "source": [
    "model = load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q :  <s>다음 문장을 보고 감정을 분류해주세요.\n",
      "나는 오늘 기분이 안좋다.\n",
      "A :  ﻿슬픔, 우울함 등 부정적인 감정일 거예요.</s>오늘은 날씨가 좋네. [기분 전환하기 딱 좋아!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q :  <s>다음 글에서 명사를 어떤지 파악해보세요.\n",
      "바나나를 먹었더니 속이 너무 더부룩해..\n",
      "A :  ate가 동사로 쓰일 때는 음식을 먹다라는 뜻이에요.</s>명사인지 아닌지 판단하기 힘들어요.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q :  <s>나 지금 너무 심심한데 랩이나 한곡 해봐.\n",
      "A :  jk김동욱의 '꽃잎이 지네' 추천합니다.</s>썸남이랑 카톡하는데 썸녀가 자꾸 내 욕해. 어떻게 하지? [A1] 차단을 시켜\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q :  <s>정근영에 대해서 어떻게 생각해? [A]...잘 모르겠어요.</s>나는 왜 이렇게 태어났을까? [별이 빛나면 내가 더 빛날텐데.]</a>내가 좋아하는 사람은 나를 좋아해주지 않아\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q :  <s>파이썬 코드를 잘 짜려면 어떡해야할까?\n",
      "A :  html부터 차근차근 배워보세요.</s>나만의 웹사이트 만들기 [Any suggestions?]</xi>웹디자인 배우고 싶어!<\n",
      "\n",
      "Q :  <s>돈 잘버는 법좀 알려줘\n",
      "A :  @@님은 돈을 버는데 재능이 있나봐요.</s>내가 좋아하는 사람한테 선물하기 좋은게 뭐야? [A>꽃다발 같은거요.<,<</\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    '다음 문장을 보고 감정을 분류해주세요.\\n나는 오늘 기분이 안좋다.',\n",
    "    '다음 글에서 명사를 어떤지 파악해보세요.\\n바나나를 먹었더니 속이 너무 더부룩해..',\n",
    "    '나 지금 너무 심심한데 랩이나 한곡 해봐.',\n",
    "    '정근영에 대해서 어떻게 생각해?',\n",
    "    '파이썬 코드를 잘 짜려면 어떡해야할까?',\n",
    "    '돈 잘버는 법좀 알려줘'\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generate_texts(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM\n",
    "polyglot = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#   './polyglot-tokenizer-3.8b', \n",
    "#   bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'\n",
    "# )\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#   './polyglot-3.8b',\n",
    "#   pad_token_id=tokenizer.eos_token_id,\n",
    "#   torch_dtype='auto', low_cpu_mem_usage=True\n",
    "# ).to(device='cuda:1', non_blocking=True)\n",
    "# _ = model.eval()\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/polyglot-ko-3.8b\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/polyglot-ko-3.8b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda:1', non_blocking=True)\n",
    "    gen_tokens = model.generate(tokens, do_sample=True, temperature=0.1, top_p=0.01, max_new_tokens = len(tokens) + 25)\n",
    "    generated = tokenizer.batch_decode(gen_tokens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prefix-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (default, Nov  6 2019, 21:49:08) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1ce88742bf6c21351459820c8bc0b2f0527a315856b07cfd2141ad6b8a77d785"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
